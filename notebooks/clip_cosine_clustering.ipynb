{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "563c42a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import is_\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../.')\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import clip\n",
    "\n",
    "from Metrics import base_kmeans_model_evaluation, kmeans_with_init, cosine_kmeans_with_init\n",
    "from networks import CustomCLIP, load_clip_to_cpu\n",
    "from lr_scheduler import ConstantWarmupScheduler\n",
    "\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20583beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14']\n",
      "Device: cuda\n",
      "Current cuda device: 0\n",
      "Count of using GPUs: 4\n"
     ]
    }
   ],
   "source": [
    "clip_backbone = \"ViT-L/14\"\n",
    "print(clip.available_models())\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else \"cpu\"\n",
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())\n",
    "\n",
    "backbone_name = clip_backbone\n",
    "clip_model, preprocess = load_clip_to_cpu(backbone_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40043852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size =50\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=preprocess)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8ac1f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing class-specific contexts\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n"
     ]
    }
   ],
   "source": [
    "model = CustomCLIP(clip_model, len(testset.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a60f0cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b56fff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ce9b66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656ddb85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "727ce65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_list = testset.classes\n",
    "classes_list = ['a photo of a ' + txt for txt in classes_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1626c0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a photo of a airplane',\n",
       " 'a photo of a automobile',\n",
       " 'a photo of a bird',\n",
       " 'a photo of a cat',\n",
       " 'a photo of a deer',\n",
       " 'a photo of a dog',\n",
       " 'a photo of a frog',\n",
       " 'a photo of a horse',\n",
       " 'a photo of a ship',\n",
       " 'a photo of a truck']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89b85617",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    classes_list = clip.tokenize(classes_list).to(device)\n",
    "    centroid_candidate_text = clip_model.to(device).encode_text(classes_list)\n",
    "    centroid_candidate_text = centroid_candidate_text / centroid_candidate_text.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb659251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_ACC 0.9535\n",
      "image_NMI 0.8978035140252928\n",
      "267.17397451400757\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "new_label, acc, nmi = cosine_kmeans_with_init(\n",
    "    model, testloader, 10, centroid_candidate_text)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7443b31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_ACC 0.9606\n",
      "image_NMI 0.9090755996653886\n",
      "301.06729388237\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "new_label, acc, nmi = kmeans_with_init(\n",
    "    model, testloader, 10, centroid_candidate_text)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "551e12c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                       download=True, transform=preprocess)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01daac91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_ACC 0.9551\n",
      "image_NMI 0.9010647389422483\n",
      "1452.4428737163544\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "new_label, acc, nmi = cosine_kmeans_with_init(\n",
    "    model, trainloader, 10, centroid_candidate_text)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ef9461e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_ACC 0.96212\n",
      "image_NMI 0.9123226975349202\n",
      "1398.7406959533691\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "new_label, acc, nmi = kmeans_with_init(\n",
    "    model, trainloader, 10, centroid_candidate_text)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198f57ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
