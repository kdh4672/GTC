{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "563c42a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import is_\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../.')\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import clip\n",
    "\n",
    "from Metrics import base_kmeans_model_evaluation, kmeans_with_init, cosine_kmeans_with_init\n",
    "from networks import CustomCLIP, load_clip_to_cpu\n",
    "from lr_scheduler import ConstantWarmupScheduler\n",
    "\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20583beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14']\n",
      "Device: cuda\n",
      "Current cuda device: 0\n",
      "Count of using GPUs: 4\n"
     ]
    }
   ],
   "source": [
    "clip_backbone = \"ViT-L/14\"\n",
    "print(clip.available_models())\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else \"cpu\"\n",
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())\n",
    "\n",
    "backbone_name = clip_backbone\n",
    "clip_model, preprocess = load_clip_to_cpu(backbone_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40043852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size =10\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=preprocess)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False)\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                       download=True, transform=preprocess)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8ac1f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing class-specific contexts\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n"
     ]
    }
   ],
   "source": [
    "model = CustomCLIP(clip_model, len(testset.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "af5aa11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_ACC 0.8359\n",
      "image_NMI 0.8406474690785359\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10 \n",
    "with torch.no_grad():\n",
    "    centroids, test_label, acc, nmi= base_kmeans_model_evaluation(\n",
    "        clip_model.to(device), testloader, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "f0a51781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_ACC 0.95328\n",
      "image_NMI 0.9012859197933579\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10 \n",
    "with torch.no_grad():\n",
    "    train_centroids, test_label, acc, nmi= base_kmeans_model_evaluation(\n",
    "        clip_model.to(device), trainloader, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "bf4bb185",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = centroids\n",
    "X_square = X**2\n",
    "X_square_sum = np.sum(X_square,axis=1)\n",
    "X_train_square = X**2\n",
    "X_square_train_sum = np.sum(X_train_square,axis=1)\n",
    "XY = X@X.T\n",
    "# dists_original = np.sqrt(X_square_sum.reshape(-1,1)+X_square_train_sum.reshape(1,-1)-2*XY)\n",
    "dists = np.sqrt(X_square_sum.reshape(-1,1)+X_square_train_sum.reshape(1,-1)-2*XY+1e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "8bd8191c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmin(dists,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f67fd33d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 0, 7, 5, 8, 9, 9, 2, 4, 5])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists.argsort(axis=-1)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "a76152c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = dists[np.arange(10),dists.argsort(axis=-1)[:,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bf32daf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0110123 ,  0.03005093, -0.01091487, ...,  0.00862239,\n",
       "         0.00132192, -0.0172122 ],\n",
       "       [ 0.00724263,  0.0220054 ,  0.00479171, ..., -0.00250016,\n",
       "        -0.00948331, -0.02184404],\n",
       "       [ 0.00571699,  0.03417663, -0.00728394, ...,  0.00621605,\n",
       "        -0.00533025, -0.02589159],\n",
       "       ...,\n",
       "       [ 0.00849806,  0.04163024,  0.00323967, ...,  0.01474007,\n",
       "         0.01407203, -0.02373944],\n",
       "       [-0.01054857,  0.02590865, -0.01196582, ...,  0.03023189,\n",
       "        -0.00372186, -0.02541586],\n",
       "       [-0.01246983,  0.03131478,  0.00270114, ...,  0.00727812,\n",
       "        -0.00256862, -0.01381275]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20acec98",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_list = ['a photo of a '+ c for c in testset.classes]\n",
    "classes_list\n",
    "with torch.no_grad():\n",
    "    classes_list = clip.tokenize(classes_list).to(device)\n",
    "    centroid_candidate_text = clip_model.to(device).encode_text(classes_list)\n",
    "    centroid_candidate_text = centroid_candidate_text / centroid_candidate_text.norm(dim=-1, keepdim=True)\n",
    "for i, (x, target) in enumerate(trainloader):\n",
    "    x = x.to(device)\n",
    "    if i ==0:\n",
    "        break\n",
    "image_features = model.image_encoder(x.type(clip_model.dtype))\n",
    "image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "sim = image_features@centroid_candidate_text.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f14cdd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6172282 , 0.06005859, 0.65896441, 0.8763368 , 0.93334016,\n",
       "        0.8563349 , 0.68656232, 0.79860074, 0.17883979, 0.14537133]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = (image_features**2).cpu().detach().numpy()\n",
    "np.square(np.sum(x,axis=-1))\n",
    "\n",
    "np.random.rand(1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "3b4e7c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = image_features.detach().cpu().numpy()\n",
    "X_square = X**2\n",
    "X_square_sum = np.sum(X_square,axis=1)\n",
    "X_train = train_centroids\n",
    "X_train_square = X_train**2\n",
    "X_square_train_sum = np.sum(X_train_square,axis=1)\n",
    "XY = X@X_train.T\n",
    "# dists_original = np.sqrt(X_square_sum.reshape(-1,1)+X_square_train_sum.reshape(1,-1)-2*XY)\n",
    "dists = np.sqrt(X_square_sum.reshape(-1,1)+X_square_train_sum.reshape(1,-1)-2*XY+1e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4840bb25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0237, -0.0017,  0.0569,  ...,  0.0322,  0.0125, -0.0069],\n",
       "        [ 0.0119, -0.0088,  0.0164,  ...,  0.0056,  0.0435, -0.0258],\n",
       "        [-0.0066,  0.0147,  0.0375,  ...,  0.0239,  0.0115,  0.0017],\n",
       "        ...,\n",
       "        [ 0.0291,  0.0064,  0.0308,  ...,  0.0106, -0.0188, -0.0157],\n",
       "        [-0.0145, -0.0089,  0.0215,  ...,  0.0299,  0.0108, -0.0028],\n",
       "        [-0.0124, -0.0029,  0.0378,  ..., -0.0095,  0.0779,  0.0179]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroid_candidate_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8a0ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = self.prompt_learner()\n",
    "tokenized_prompts = self.tokenized_prompts\n",
    "text_features = self.text_encoder(prompts, tokenized_prompts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
