{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c96b7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import is_\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../.')\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import clip\n",
    "\n",
    "from Metrics import base_kmeans_model_evaluation, kmeans_with_init, cosine_kmeans_with_init\n",
    "from networks import CustomCLIP, load_clip_to_cpu\n",
    "from lr_scheduler import ConstantWarmupScheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import argparse\n",
    "\n",
    "class custom_dataset(Dataset): \n",
    "    def __init__(self,  root_dir='../data/imagenet10', transform=None): \n",
    "        self.transform = transform\n",
    "        data = np.load(os.path.join(root_dir,'data.npy'))\n",
    "        self.targets = np.load(os.path.join(root_dir,'label.npy'))\n",
    "        self.classes = list(set(self.targets))\n",
    "        # b,h,w,c = data.shape\n",
    "        # data = data.reshape((b, c, h, w))\n",
    "        # self.data = data.transpose((0, 2, 3, 1))\n",
    "        self.data = data\n",
    "                \n",
    "    def __getitem__(self, index):\n",
    "        img, label = self.data[index], self.targets[index]\n",
    "        img = Image.fromarray(img)\n",
    "        img = self.transform(img) \n",
    "        return img,label\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "def dataset_select(dataset_name):\n",
    "    if dataset_name == 'imagenet_dog':\n",
    "        is_test_dataset = True\n",
    "        fixed_dataset = custom_dataset(root_dir='../data/imagenet_dog', transform=preprocess)\n",
    "        fixed_dataloader = DataLoader(\n",
    "            dataset=fixed_dataset, \n",
    "            batch_size=batch_size,\n",
    "            shuffle=False)\n",
    "        dataset = custom_dataset(root_dir='../data/imagenet_dog', transform=preprocess)\n",
    "        classes = ['cavachon','cavalier king charles spaniel','Basset Hound','Norwegian Elkhound',\\\n",
    "                  'Giant Schnauzer','golden retriever','brittany spaniel','clumber spaniel',\\\n",
    "                  'cocker spaniel','Belgian Shepherd Groenendael','Lancashire Heeler','rough collie','doberman pinscher','pug','chow chow']\n",
    "    elif dataset_name == 'stanford40':\n",
    "        is_test_dataset = True\n",
    "        fixed_dataset = custom_dataset(root_dir='/database/daehyeon/Stanford40/JPEGImages', transform=preprocess)\n",
    "        fixed_dataloader = DataLoader(\n",
    "            dataset=fixed_dataset, \n",
    "            batch_size=batch_size,\n",
    "            shuffle=False)\n",
    "        dataset = custom_dataset(root_dir='/database/daehyeon/Stanford40/JPEGImages', transform=preprocess)\n",
    "        classes = ['rowing a boat', 'blowing bubbles', 'phoning', 'playing guitar', 'fishing', 'pouring liquid', 'waving hands', 'writing on a book', 'holding an umbrella', 'riding a horse', 'washing dishes', 'fixing a bike', 'running', 'fixing a car', 'brushing teeth', 'climbing', 'cooking', 'jumping', 'looking through a microscope', 'feeding a horse', 'using a computer', 'reading', 'riding a bike', 'walking the dog', 'gardening', 'texting message', 'throwing frisby', 'cleaning the floor', 'drinking', 'pushing a cart', 'watching TV', 'cutting trees', 'looking through a telescope', 'cutting vegetables', 'shooting an arrow', 'smoking', 'applauding', 'writing on a board', 'taking photos', 'playing violin']\n",
    "\n",
    "    elif dataset_name == 'mit':\n",
    "        is_test_dataset = True\n",
    "        fixed_dataset = custom_dataset(root_dir='/database/daehyeon/mit', transform=preprocess)\n",
    "        fixed_dataloader = DataLoader(\n",
    "            dataset=fixed_dataset, \n",
    "            batch_size=batch_size,\n",
    "            shuffle=False)\n",
    "        dataset = custom_dataset(root_dir='/database/daehyeon/mit', transform=preprocess)\n",
    "        classes = ['poolinside','bedroom','inside_subway','operating_room','warehouse','clothingstore','toystore','waitingroom','lobby','casino','subway','closet','livingroom','laundromat','studiomusic','fastfood_restaurant','buffet','grocerystore','bakery','nursery','florist','airport_inside','artstudio','laboratorywet','videostore','kitchen','elevator','mall','deli','dentaloffice','greenhouse','bar','prisoncell','children_room','museum','gym','office','jewelleryshop','locker_room','hairsalon','restaurant','trainstation','pantry','dining_room','meeting_room','bookstore','tv_studio','bathroom','auditorium','bowling','shoeshop','restaurant_kitchen','hospitalroom','concert_hall','stairscase','corridor','garage','inside_bus','winecellar','computerroom','cloister','movietheater','kindergarden','library','classroom','gameroom','church_inside']\n",
    "    return dataset,classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d1c64e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing class-specific contexts\n",
      "Initial context: \"age of personX X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "backbone_name = \"ViT-B/16\"\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = load_clip_to_cpu(backbone_name)\n",
    "n_ctx=16\n",
    "dataset_name = 'imagenet_dog'\n",
    "\n",
    "### dataset \n",
    "\n",
    "num_classes= len(testset.classes)\n",
    "model = CustomCLIP(clip_model, num_classes, n_ctx=n_ctx)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if \"prompt_learner\" not in name:\n",
    "        param.requires_grad_(False)\n",
    "\n",
    "model.to(device)\n",
    "print('done')\n",
    "#mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58878473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['airplane',\n",
       " 'automobile',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'deer',\n",
       " 'dog',\n",
       " 'frog',\n",
       " 'horse',\n",
       " 'ship',\n",
       " 'truck']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size =100\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=None)\n",
    "testset.classes\n",
    "# testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "#                                          shuffle=False)\n",
    "\n",
    "# trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "#                                        download=True, transform=preprocess)\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "#                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdb03a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# batch_size = 50\n",
    "# trainset = torchvision.datasets.STL10(\n",
    "#     root='../data', split='train', download=True,\n",
    "#     transform=preprocess)\n",
    "# testset = torchvision.datasets.STL10(\n",
    "#     root='../data', split='test', download=True,\n",
    "#     transform=preprocess)\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "#                                           shuffle=False)\n",
    "# testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "#                                           shuffle=False)\n",
    "# concatset = torch.utils.data.ConcatDataset([trainset, testset])\n",
    "# concatloader = torch.utils.data.DataLoader(concatset, batch_size=batch_size,\n",
    "#                                          shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3940555a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b6ba7da1",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be str, not numpy.int64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-8d0d7299b3f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclasses_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'a photo of a '\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtestset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mclasses_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcentroid_candidate_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-8d0d7299b3f6>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclasses_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'a photo of a '\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtestset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mclasses_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcentroid_candidate_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: must be str, not numpy.int64"
     ]
    }
   ],
   "source": [
    "classes_list = ['a photo of a '+ c for c in testset.classes]\n",
    "print(classes_list)\n",
    "with torch.no_grad():\n",
    "    classes_list = clip.tokenize(classes_list).to(device)\n",
    "    centroid_candidate_text = clip_model.to(device).encode_text(classes_list)\n",
    "    centroid_candidate_text = centroid_candidate_text / centroid_candidate_text.norm(dim=-1, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db53bb35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Initializing class-specific contexts\n",
      "Initial context: \"age of personX X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "['a photo of a airplane', 'a photo of a automobile', 'a photo of a bird', 'a photo of a cat', 'a photo of a deer', 'a photo of a dog', 'a photo of a frog', 'a photo of a horse', 'a photo of a ship', 'a photo of a truck']\n",
      "0 / 1000\n",
      "50 / 1000\n",
      "100 / 1000\n",
      "150 / 1000\n",
      "200 / 1000\n",
      "250 / 1000\n",
      "300 / 1000\n",
      "350 / 1000\n",
      "400 / 1000\n",
      "450 / 1000\n",
      "500 / 1000\n",
      "550 / 1000\n",
      "600 / 1000\n",
      "650 / 1000\n",
      "700 / 1000\n",
      "750 / 1000\n",
      "800 / 1000\n",
      "850 / 1000\n",
      "900 / 1000\n",
      "950 / 1000\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "image_ACC 0.9072666666666667\n",
      "image_NMI 0.8204888381567208\n",
      "image_ARI 0.8080968944748131\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-5\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "dataset_name_list = ['cifar10']\n",
    "for dataset_name in dataset_name_list:\n",
    "    clip_backbone = \"ViT-B/16\"\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else \"cpu\"\n",
    "    backbone_name = clip_backbone\n",
    "    clip_model, preprocess = load_clip_to_cpu(backbone_name)\n",
    "\n",
    "#     testset, classes = dataset_select(dataset_name)\n",
    "    batch_size =100\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                           download=True, transform=preprocess)\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                           download=True, transform=preprocess)\n",
    "    concatset = torch.utils.data.ConcatDataset([trainset, testset])\n",
    "    concatloader = torch.utils.data.DataLoader(concatset, batch_size=batch_size,\n",
    "                                             shuffle=False)\n",
    "    classes = testset.classes\n",
    "    \n",
    "    model = CustomCLIP(clip_model, len(classes))\n",
    "    model.to(device)\n",
    "\n",
    "    classes_list = ['a photo of a '+ c for c in classes]\n",
    "    print(classes_list)\n",
    "    with torch.no_grad():\n",
    "        classes_list = clip.tokenize(classes_list).to(device)\n",
    "        centroid_candidate_text = clip_model.to(device).encode_text(classes_list)\n",
    "        centroid_candidate_text = centroid_candidate_text / centroid_candidate_text.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        weight_decay=5e-4,\n",
    "        betas=(0.9, 0.999),\n",
    "    )\n",
    "    for i in range(1000):\n",
    "        if i%50 == 0:\n",
    "            print(i,'/',1000)\n",
    "        prompts = model.prompt_learner().cuda()\n",
    "        tokenized_prompts = model.tokenized_prompts\n",
    "        for j in range(len(prompts)):\n",
    "            text_feature = model.text_encoder(prompts[j], tokenized_prompts)\n",
    "            text_feature = text_feature / \\\n",
    "                text_feature.norm(dim=-1, keepdim=True)\n",
    "            if j == 0:\n",
    "                text_features = text_feature.expand(1, -1, -1)\n",
    "            else:\n",
    "                text_feature = text_feature.expand(1, -1, -1)\n",
    "                text_features = torch.cat((text_features, text_feature), dim=0)\n",
    "        text_features = text_features.mean(dim=0)\n",
    "\n",
    "        loss = criterion(text_features,centroid_candidate_text)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    num_classes = 10\n",
    "    with torch.no_grad():\n",
    "        knn = cosine_kmeans_with_init\n",
    "        new_label, acc, nmi,ari = knn(\n",
    "            model, concatloader, num_classes, text_features)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd61ad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"acc,nmi,ari:\",acc,nmi,ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3b4b2d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing class-specific contexts\n",
      "Initial context: \"age of personX X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "['a photo of a poolinside', 'a photo of a bedroom', 'a photo of a inside_subway', 'a photo of a operating_room', 'a photo of a warehouse', 'a photo of a clothingstore', 'a photo of a toystore', 'a photo of a waitingroom', 'a photo of a lobby', 'a photo of a casino', 'a photo of a subway', 'a photo of a closet', 'a photo of a livingroom', 'a photo of a laundromat', 'a photo of a studiomusic', 'a photo of a fastfood_restaurant', 'a photo of a buffet', 'a photo of a grocerystore', 'a photo of a bakery', 'a photo of a nursery', 'a photo of a florist', 'a photo of a airport_inside', 'a photo of a artstudio', 'a photo of a laboratorywet', 'a photo of a videostore', 'a photo of a kitchen', 'a photo of a elevator', 'a photo of a mall', 'a photo of a deli', 'a photo of a dentaloffice', 'a photo of a greenhouse', 'a photo of a bar', 'a photo of a prisoncell', 'a photo of a children_room', 'a photo of a museum', 'a photo of a gym', 'a photo of a office', 'a photo of a jewelleryshop', 'a photo of a locker_room', 'a photo of a hairsalon', 'a photo of a restaurant', 'a photo of a trainstation', 'a photo of a pantry', 'a photo of a dining_room', 'a photo of a meeting_room', 'a photo of a bookstore', 'a photo of a tv_studio', 'a photo of a bathroom', 'a photo of a auditorium', 'a photo of a bowling', 'a photo of a shoeshop', 'a photo of a restaurant_kitchen', 'a photo of a hospitalroom', 'a photo of a concert_hall', 'a photo of a stairscase', 'a photo of a corridor', 'a photo of a garage', 'a photo of a inside_bus', 'a photo of a winecellar', 'a photo of a computerroom', 'a photo of a cloister', 'a photo of a movietheater', 'a photo of a kindergarden', 'a photo of a library', 'a photo of a classroom', 'a photo of a gameroom', 'a photo of a church_inside']\n",
      "0 / 1000\n",
      "50 / 1000\n",
      "100 / 1000\n",
      "150 / 1000\n",
      "200 / 1000\n",
      "250 / 1000\n",
      "300 / 1000\n",
      "350 / 1000\n",
      "400 / 1000\n",
      "450 / 1000\n",
      "500 / 1000\n",
      "550 / 1000\n",
      "600 / 1000\n",
      "650 / 1000\n",
      "700 / 1000\n",
      "750 / 1000\n",
      "800 / 1000\n",
      "850 / 1000\n",
      "900 / 1000\n",
      "950 / 1000\n",
      "image_ACC 0.23925\n",
      "image_NMI 0.22290412803813067\n",
      "image_ARI 0.07861327529323159\n",
      "Initializing class-specific contexts\n",
      "Initial context: \"age of personX X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "['a photo of a rowing a boat', 'a photo of a blowing bubbles', 'a photo of a phoning', 'a photo of a playing guitar', 'a photo of a fishing', 'a photo of a pouring liquid', 'a photo of a waving hands', 'a photo of a writing on a book', 'a photo of a holding an umbrella', 'a photo of a riding a horse', 'a photo of a washing dishes', 'a photo of a fixing a bike', 'a photo of a running', 'a photo of a fixing a car', 'a photo of a brushing teeth', 'a photo of a climbing', 'a photo of a cooking', 'a photo of a jumping', 'a photo of a looking through a microscope', 'a photo of a feeding a horse', 'a photo of a using a computer', 'a photo of a reading', 'a photo of a riding a bike', 'a photo of a walking the dog', 'a photo of a gardening', 'a photo of a texting message', 'a photo of a throwing frisby', 'a photo of a cleaning the floor', 'a photo of a drinking', 'a photo of a pushing a cart', 'a photo of a watching TV', 'a photo of a cutting trees', 'a photo of a looking through a telescope', 'a photo of a cutting vegetables', 'a photo of a shooting an arrow', 'a photo of a smoking', 'a photo of a applauding', 'a photo of a writing on a board', 'a photo of a taking photos', 'a photo of a playing violin']\n",
      "0 / 1000\n",
      "50 / 1000\n",
      "100 / 1000\n",
      "150 / 1000\n",
      "200 / 1000\n",
      "250 / 1000\n",
      "300 / 1000\n",
      "350 / 1000\n",
      "400 / 1000\n",
      "450 / 1000\n",
      "500 / 1000\n",
      "550 / 1000\n",
      "600 / 1000\n",
      "650 / 1000\n",
      "700 / 1000\n",
      "750 / 1000\n",
      "800 / 1000\n",
      "850 / 1000\n",
      "900 / 1000\n",
      "950 / 1000\n",
      "image_ACC 0.390875\n",
      "image_NMI 0.43236142651216286\n",
      "image_ARI 0.24749154604257664\n",
      "Initializing class-specific contexts\n",
      "Initial context: \"age of personX X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "['a photo of a cavachon', 'a photo of a cavalier king charles spaniel', 'a photo of a Basset Hound', 'a photo of a Norwegian Elkhound', 'a photo of a Giant Schnauzer', 'a photo of a golden retriever', 'a photo of a brittany spaniel', 'a photo of a clumber spaniel', 'a photo of a cocker spaniel', 'a photo of a Belgian Shepherd Groenendael', 'a photo of a Lancashire Heeler', 'a photo of a rough collie', 'a photo of a doberman pinscher', 'a photo of a pug', 'a photo of a chow chow']\n",
      "0 / 1000\n",
      "50 / 1000\n",
      "100 / 1000\n",
      "150 / 1000\n",
      "200 / 1000\n",
      "250 / 1000\n",
      "300 / 1000\n",
      "350 / 1000\n",
      "400 / 1000\n",
      "450 / 1000\n",
      "500 / 1000\n",
      "550 / 1000\n",
      "600 / 1000\n",
      "650 / 1000\n",
      "700 / 1000\n",
      "750 / 1000\n",
      "800 / 1000\n",
      "850 / 1000\n",
      "900 / 1000\n",
      "950 / 1000\n",
      "image_ACC 0.290625\n",
      "image_NMI 0.17682529125753146\n",
      "image_ARI 0.0865837566793079\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-6\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "dataset_name_list = ['mit','stanford40','imagenet_dog']\n",
    "for dataset_name in dataset_name_list:\n",
    "    clip_backbone = \"ViT-B/16\"\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else \"cpu\"\n",
    "    backbone_name = clip_backbone\n",
    "    clip_model, preprocess = load_clip_to_cpu(backbone_name)\n",
    "\n",
    "    testset, classes = dataset_select(dataset_name)\n",
    "    \n",
    "    model = CustomCLIP(clip_model, len(classes))\n",
    "    model.to(device)\n",
    "\n",
    "    classes_list = ['a photo of a '+ c for c in classes]\n",
    "    print(classes_list)\n",
    "    with torch.no_grad():\n",
    "        classes_list = clip.tokenize(classes_list).to(device)\n",
    "        centroid_candidate_text = clip_model.to(device).encode_text(classes_list)\n",
    "        centroid_candidate_text = centroid_candidate_text / centroid_candidate_text.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        weight_decay=5e-4,\n",
    "        betas=(0.9, 0.999),\n",
    "    )\n",
    "    for i in range(1000):\n",
    "        if i%50 == 0:\n",
    "            print(i,'/',1000)\n",
    "        prompts = model.prompt_learner().cuda()\n",
    "        tokenized_prompts = model.tokenized_prompts\n",
    "        for j in range(len(prompts)):\n",
    "            text_feature = model.text_encoder(prompts[j], tokenized_prompts)\n",
    "            text_feature = text_feature / \\\n",
    "                text_feature.norm(dim=-1, keepdim=True)\n",
    "            if j == 0:\n",
    "                text_features = text_feature.expand(1, -1, -1)\n",
    "            else:\n",
    "                text_feature = text_feature.expand(1, -1, -1)\n",
    "                text_features = torch.cat((text_features, text_feature), dim=0)\n",
    "        text_features = text_features.mean(dim=0)\n",
    "\n",
    "        loss = criterion(text_features,centroid_candidate_text)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        knn = cosine_kmeans_with_init\n",
    "        new_label, acc, nmi,ari = knn(\n",
    "            model, testloader, num_classes, text_features)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e5d3f933",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['poolinside','bedroom','inside_subway','operating_room','warehouse','clothingstore','toystore','waitingroom','lobby','casino','subway','closet','livingroom','laundromat','studiomusic','fastfood_restaurant','buffet','grocerystore','bakery','nursery','florist','airport_inside','artstudio','laboratorywet','videostore','kitchen','elevator','mall','deli','dentaloffice','greenhouse','bar','prisoncell','children_room','museum','gym','office','jewelleryshop','locker_room','hairsalon','restaurant','trainstation','pantry','dining_room','meeting_room','bookstore','tv_studio','bathroom','auditorium','bowling','shoeshop','restaurant_kitchen','hospitalroom','concert_hall','stairscase','corridor','garage','inside_bus','winecellar','computerroom','cloister','movietheater','kindergarden','library','classroom','gameroom','church_inside']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e413ade2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "['rowing a boat', 'blowing bubbles', 'phoning', 'playing guitar', 'fishing', 'pouring liquid', 'waving hands', 'writing on a book', 'holding an umbrella', 'riding a horse', 'washing dishes', 'fixing a bike', 'running', 'fixing a car', 'brushing teeth', 'climbing', 'cooking', 'jumping', 'looking through a microscope', 'feeding a horse', 'using a computer', 'reading', 'riding a bike', 'walking the dog', 'gardening', 'texting message', 'throwing frisby', 'cleaning the floor', 'drinking', 'pushing a cart', 'watching TV', 'cutting trees', 'looking through a telescope', 'cutting vegetables', 'shooting an arrow', 'smoking', 'applauding', 'writing on a board', 'taking photos', 'playing violin']\n"
     ]
    }
   ],
   "source": [
    "list_ = os.listdir('/database/daehyeon/temporary/')\n",
    "dict_ = {}\n",
    "for item in list_:\n",
    "    a = item.split('.')[0]\n",
    "    b = a.split('_')[:-1]\n",
    "    c = ' '.join(b)\n",
    "    dict_[c] = 1\n",
    "classes = list(dict_.keys())\n",
    "print(len(classes))\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c58d4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
